This is Dr Chao-yo Cheng -- welcome back to IQSR -- I hope you are doing well.

***

Last week -- I started introducing you all to the analysis of survey data -- I mentioned the components of survey design and explained why the design is a very crucial issues -- then in the live session we had a tutorial so you have learned how to carry out some basic descriptive analysis of survey data in R -- this week we will continue the topic on survey data analysis and we will focus on the inferential aspect of survey data analysis -- that is -- we will how to use regression models to analyze survey data -- in particular we will be using logit regression as the example

So in the second part of the lecture video we will have a really quick review of linear regression and logit regression

***

This is the outline for today's lecture -- we will start with a brief review of what we covered in the previous week -- then I will talk about the options we have when it comes to survey data analysis -- mostly, the decision we have to make largely depends on the specific type of outcome variables we have -- in the next video, we will review the materials from the first term on linear regression and logit regression -- we will pay particular attention to two things -- one, we will review the connection between linear and logit models -- i will try to explain why it might be a good idea to use logit and other generalized linear models to analyze binary outcomes -- then the other task for the review is to make sure everyone is familiar with the underlying idea of the odds ratio and able to use the odds ratio to interpret the analytical results produced by logit regression

In the online session, we will use the same data from the previous week -- the 2011 Canadian National Election Study -- this time, we will try to use one or two predictors to study people's attitudes toward abortion

***

In this video, we are going to review the materials from Term 1 through three steps.

First, we will have a really quick review on OLS linear regression or the so-called ordinary least squares.

Next, I will explain why many researchers believe logit and other generalized linear models are a better choice when we encounter outcome variables that can only take two values, such as 0 and 1.

Finally, we will review the mechanics of logit regression -- including things like the basic setup of logit regression and how we understand or interpret the results f 

We will not talk about the required assumptions, model diagnostics, and model comparison in this video, but I am going cover these topics in the review workshops. And you are very welcomed to join the workshops with your questions.

***

By def, linear regression is a very attractive idea -- by doing linear regression analysis, it basically means we want to use a linear function to model or represent the statistical relationship between the explanatory and dependent variables.

A simple bivariate linear regression can rewritten like the following equation [equation]

In the model, alpha is intercept and beta is the slope.

Ordinary least squares is perhaps the most common linear regression technique we use. OLS allows us to find the intercept and slopes to create a line such that the fitted regression line will produce the smallest errors based on the data we have -- to be more specific, by errors we mean the sum of squared errors, and this is also where the name of ordinary least squares came from

***

This is a very typical visual illustration of a bivariate linear regression -- I will encourage everyone to go through the scatterplot here carefully and make sure you understand each part -- if you have any questions, feel free to bring them up in the live session.  I will be very brief.

So the first thing we should notice is X and Y, which refer to the predictor and the outcome variable.

Then each point refers to an individual observation in the dataset.

The line is the fitted regression line -- you perhaps notice that the intercept of this fitted line is negative and the slope should positive.

The distance between each point and the fitted line is the corresponding unique residual or error for that particular data point or observation.

***

This is a very quick example of a bivariate linear regression -- bivariate means we have one dependent variable and one predictor or explanatory variable.

For this particular linear regression, we are trying to understand the statistical relationship between the number of hours a politician spends on campaigns during the elections and the percentage vote the politician will get.

Suppose this is the model we get from linear regression analysis, we can try to predict a politician's performance in the elections based on the number of hours they spend on campaigns. For instance, when the politician does not do anything, they will get 50% of votes, likewise, if the politician only spends on hour, they will get 52% of votes.

50 is the intercept; basically it means the party or the candidate will get 50% of votes without doing anything.

2 is the slope of this linear regression model; some people may say 2 is the marginal effect (or the rate of change) of the time spent on campaigning -- because it simply says: by spending one additional hour doing campaigns, the party or the politician can increase its vote share by 2 percentage points.  You can see that by taking the difference between 50 and 52. 

***

That was a very very quick recap on linear regression.  

While linear regression or ordinary least squares is very powerful very useful. It may create problems when we encounter some specific types of dependent variables -- more specifically, if the outcome variable is either binary or bounded, then we have think again if linear regression is still the ideal modeling choice.

By binary, we mean the variable can only take two values or levels, say if a person lives in London. If we use 0 and 1 to indicate a person lives in the city or the country, then linear regression may produce predictions below 0 or larger than 1, both of which do not make any sense at all from the probability point of view.

By bounded, we mean the variable has some natural limits given the way we set up the measurement -- for instance, using example from the previous slide, we will see that a politician is going to get more than 100% of votes if they spend 30 hours campaigning and we know that this is impossible in real life.

We can also try to explain why linear regression is problematic for binary or bounded dependent variables by looking at the required assumptions for ordinary least squares, and you should have been more or less familiar with these assumptions. To put this argument in a relatively simply way, if we use linear regression to model or analyze binary dependent variables, the linear regression model may produce residuals or errors that behave in a way that violates the typical assumptions we need for linear regression.

If you want to see a more detailed explanation on the second point, the book I list might be useful.

And we will have a more extensive discussion on the second point in the review workshop. 

***

Logit regression is invented or created to analyze binary variables. In the final part of this lecture, I will explain why logit regression can be a choice for this objective and I will talk about how to understand the results produced by logit regression.

Before we begin, here is couple of reminders

First, usually in practice, binary variables will take the values of 0 and 1. We use 0-1 binary or dummy variables to specify whether or not something happens. Again, for instance, we can create a binary var to indicate whether or not a person voted for Donald Trump in the presidential election or depending on your research questions, we can use a binary variable to indicate if a person lives in England or not.

The ideas of probability here comes handy. By studying binary outcomes, our main analytical interest will be how likely something is going to happen. So many models for binary outcomes have to do with the modeling of probabilities. Given that we are talking about probabilities, then two things you should know

First -- a probability, which very often is denoted by the lower p here, can only be a number between 0 and 1.

Second -- the probability of something happening and the probability of something not happening will sum up to.  Say the probability of raining is 0.7 and then the probability of not raining is 0.3.

Also, the reason we say logit regression is a generalized linear model or GLM because it means we are using a linear function like alpha + beta times X + error to model a non-linear statistical relationship between X and the outcome

***

Let's take a look at this example -- in this example, we are interested in learning the statistical association between the horsepower of a car and whether not the car will have this particular type of engine -- so the dependent variable is binary and the predictor or the explanatory variable is continuous

This is the fitted line from logit regression -- as you can see, this is not a straight line -- because by doing logit regression, the y or vertical axis basically says the probability of a car having a straight engine, conditional on the horsepower of the car.

***

Formally speaking, logit regression is a regression technique -- we use the logit link to turn a probability into log odds; and instead of using the linear function alpha plus beta X plus epsilon to model the probability, we are now using the linear function to model the log odds of a probability.

There is a lot to unpack here. So we will do this step by step -- let's start from the left-hand side before we move to the right hand side.

*** 

Again, the logit link function will turn a probability into a log odds, which is the log of odds. Let's go through each of them one by one with this example.

Suppose today the probability of raining is 0.8 and then the probability of not raining is 0.2.

The odds is defined as p divided by 1-p; it is basically a metric for us to evaluate the chance of something happening relative to its opposite scenario -- using the same example again, it means the odds of raining is the prob of raining divided by the prob of not raining -- it basically say the probability of raining is four times the probability of not raining

The log odds is the log of odds -- in this case, it will be the log of 4 and typically we will use this particular mathematical constant lower e as the base.

Maybe I can say a bit more about why logit regression is useful -- by using the logit link function to transform a probability into the log of odds and use a linear function to model the log of odds, we are able to get around the issues we mentioned earlier -- for one thing, log-odds does not have to be between 0 and 1 and it can a negative number. So simply put, the logit link function basically turns the bounded probability into an unconstrained continuous variable that we can model with a linear function.

***

Now let's move on to the right hand side of the equation, which is a linear function with one predictor. Again, you can treat logit regression as a special case of linear regression -- the most important difference is perhaps now the dependent variable can only be the log-odds.

beta is still the slope -- so we know when we increase the predictor or X by one unit, we should see a corresponding beta change in log-odds -- in other words, we can write beta as the difference between two log odds -- the log odds when X=1 minus the log-odds when X=0.

By using the laws of logarithms, we can convert this expression into the log of of the ratio of two different odds -- the odds when X=1 divided by the odds when X=0. 

If we take the exponent of this log, we can get odds ratio -- this is a very concept or mathematical fact. In the case of logit regression,  we usually have to use odds-ratio to interpret our findings.  So we are going to use a real example to walk through these different concepts again. Intuitively speaking, odds ratio will tell us how the odds of the outcome is going change when we increase the predictor or X by one additional unit.

Again, to sum up our discussion, the slope means the corresponding change in the log-odds when we increase the predictor by one additional unit; if we take the exponent of the slope, we will get odds-ratio.

*** 

Let's take a look at this example. Say the dependent variable is binary and we assign the value of 1 to a politician if the politician was elected (otherwise we will them 0).

So we can use logit and other GLMs to model the dependent variable as a probability of whether or not a politician is going to win the election or not. If we decide to use logit regression, it means our dependent variable is the log-odds of a politician winning the election, and we can represent the log-adds with a linear function of Hours.

So to start, the slope is 0.33. It means when the politician or the party spends one additional hour on campaigns, they change log-odds of winning by 0.33. If we simply report this finding, then it is very useful because we will not be able to provide a substantive interpretation of this particular number.

Let's turn the slope into odds-ratio by taking the exponent of 0.33, which will be about 1.4.

What does this number mean? And should the party invest more time on campaigns?

***

Well, remember odds ratio is trying compare how the odds of the outcome will change when we increase the predictor by one unit. In this case, odds ratio is going tell us how the odds of winning the election will change when the party or the politician spends one additional hour on campaigns.

Even though to many people think odds-ratio is confusing, it is actually a useful statistical metric for us to understand the results produced logit regression because any odds-ratio can only be one of the three scenarios I put in the table.

If the odds-ratio is 1, then it means the odds of winning is identical when the party spends one additional hour, and we can derive the conclusion that spending more hours on campaigns is going to change the chance of winning, so the data we have do not suggest any statistical relationship between the predictor and the outcome.

If the odds-ratio is larger than one, then we know it must be the case that the numerator in the ratio is larger than the denominator, which means the odds of winning when hours = 1 is larger than the odds of winning when hours = 0; we can use this mathematical fact to conclude that spending one additional hour is going to increase the change of winning the election. As a result investing more time on campaigns is a good idea for the party or the candidate. And this is what we find based on the model we have.

***

In the live session, we are going use the 2011 CNES to demonstrate how we can use logit regression to study why some respondents were against abortion. Then we will move on to use the same survey and surveyor packagers to carry out the same analysis but now include survey weights or sampling weight into the regression analysis.

Thank you very much for watching.  See you in class.

